World Happiness Classification - Advanced Machine Learning Project
Project Overview
This project is part of an advanced machine learning course, aiming to classify countries based on happiness scores using World Happiness Report 2023 data. The goal is to apply various machine learning techniques, improve model accuracy, and document the entire process.

Team Members
Yizhao Hong
Jason Lee
Rick Hua
Repository Structure

ğŸ“‚ Advanced-Machine-Learning-World-Happiness-Classification
â”‚â”€â”€ ğŸ“„ README.md           # Project documentation
â”‚â”€â”€ ğŸ“„ requirements.txt     # Required Python dependencies
â”‚â”€â”€ ğŸ“‚ data                # Raw and processed datasets
â”‚â”€â”€ ğŸ“‚ notebooks           # Jupyter notebooks for analysis & modeling
â”‚â”€â”€ ğŸ“‚ models              # Saved trained models
â”‚â”€â”€ ğŸ“‚ src                 # Python scripts for data preprocessing & modeling
â”‚â”€â”€ ğŸ“‚ results             # Evaluation results & visualizations
â”‚â”€â”€ ğŸ“„ report.pdf          # Final report
Project Objectives
âœ” Understand model functionalities and hyperparameters
âœ” Perform data preprocessing and feature engineering
âœ” Train various classification models (Random Forest, Gradient Boosting, Deep Learning)
âœ” Optimize models using GridSearchCV
âœ” Evaluate model performance using accuracy metrics

Data Processing & Feature Engineering
Dataset
World Happiness Report 2023
Additional socioeconomic data
Key Preprocessing Steps
âœ… Handle missing values using SimpleImputer
âœ… Normalize numerical features with StandardScaler
âœ… Convert categorical variables using OneHotEncoder
âœ… Create interaction features (poverty ratio, education gap, etc.)

Machine Learning Models
Baseline Model: Random Forest
RandomForestClassifier(n_estimators=100, random_state=42)
Accuracy: 57.1%
Optimized Model: Random Forest (Tuned)
n_estimators=500, max_depth=10, max_features='log2'
Accuracy: 64.3%
Hyperparameter Optimization using GridSearchCV
Tested multiple values for max_depth, n_estimators, min_samples_split, etc.
Best model accuracy: 58.9%
Gradient Boosting Model
GradientBoostingClassifier(n_estimators=300, max_depth=6, learning_rate=0.1)
Accuracy: 50%
Deep Learning Model
Neural Network (Keras & TensorFlow)
Architecture:
4 Hidden Layers with ReLU activation
Output layer with Softmax activation for multi-class classification
Training Details:
loss='categorical_crossentropy'
optimizer='sgd' (Stochastic Gradient Descent)
Trained for 300 epochs
Key Insights & Learnings
ğŸ”¹ Feature Engineering significantly impacts model performance
ğŸ”¹ Hyperparameter Tuning improved accuracy from 57.1% â†’ 64.3%
ğŸ”¹ Deep Learning overfitting â€“ model achieved 100% accuracy in training but performed poorly in validation
ğŸ”¹ Using Adam Optimizer instead of SGD could improve deep learning training efficiency

How to Run the Project
1ï¸âƒ£ Clone the repository

git clone https://github.com/hongyizhao00/Advanced-Machine-Learning-World-Happiness-Classification-Competition.git
cd Advanced-Machine-Learning-World-Happiness-Classification-Competition
2ï¸âƒ£ Install dependencies


pip install -r requirements.txt
3ï¸âƒ£ Run Jupyter Notebook for EDA & Model Training


jupyter notebook notebooks/World_Happiness_Classification.ipynb
4ï¸âƒ£ Run the trained model for predictions


python src/predict.py
Future Improvements
ğŸ“Œ Apply ensemble methods (XGBoost, LightGBM)
ğŸ“Œ Try transfer learning techniques for deep learning models
ğŸ“Œ Perform additional hyperparameter tuning

Contributors
âœ Yizhao Hong, Jason Lee, Rick Hua

