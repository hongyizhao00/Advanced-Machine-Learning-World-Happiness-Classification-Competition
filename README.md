World Happiness Classification - Advanced Machine Learning Project

Project Overview

This project is part of an advanced machine learning course, aiming to classify countries based on happiness scores using World Happiness Report 2023 data. The goal is to apply various machine learning techniques, improve model accuracy, and document the entire process.

Team Members

Yizhao Hong

Jason Lee

Rick Hua

Repository Structure

ðŸ“‚ Advanced-Machine-Learning-World-Happiness-Classification
â”‚â”€â”€ ðŸ“„ README.md           # Project documentation
â”‚â”€â”€ ðŸ“„ requirements.txt     # Required Python dependencies
â”‚â”€â”€ ðŸ“‚ data                # Raw and processed datasets
â”‚â”€â”€ ðŸ“‚ notebooks           # Jupyter notebooks for analysis & modeling
â”‚â”€â”€ ðŸ“‚ models              # Saved trained models
â”‚â”€â”€ ðŸ“‚ src                 # Python scripts for data preprocessing & modeling
â”‚â”€â”€ ðŸ“‚ results             # Evaluation results & visualizations
â”‚â”€â”€ ðŸ“„ report.pdf          # Final report

Project Objectives

âœ” Understand model functionalities and hyperparameters
âœ” Perform data preprocessing and feature engineering
âœ” Train various classification models (Random Forest, Gradient Boosting, Deep Learning)
âœ” Optimize models using GridSearchCV
âœ” Evaluate model performance using accuracy metrics

Data Processing & Feature Engineering
Dataset
World Happiness Report 2023
Additional socioeconomic data

Key Preprocessing Steps
âœ… Handle missing values using SimpleImputer
âœ… Normalize numerical features with StandardScaler
âœ… Convert categorical variables using OneHotEncoder
âœ… Create interaction features (poverty ratio, education gap, etc.)

Machine Learning Models
Baseline Model: Random Forest
RandomForestClassifier(n_estimators=100, random_state=42)
Accuracy: 57.1%
Optimized Model: Random Forest (Tuned)
n_estimators=500, max_depth=10, max_features='log2'
Accuracy: 64.3%
Hyperparameter Optimization using GridSearchCV
Tested multiple values for max_depth, n_estimators, min_samples_split, etc.
Best model accuracy: 58.9%
Gradient Boosting Model
GradientBoostingClassifier(n_estimators=300, max_depth=6, learning_rate=0.1)
Accuracy: 50%
Deep Learning Model
Neural Network (Keras & TensorFlow)
Architecture:
4 Hidden Layers with ReLU activation
Output layer with Softmax activation for multi-class classification
Training Details:
loss='categorical_crossentropy'
optimizer='sgd' (Stochastic Gradient Descent)
Trained for 300 epochs
Key Insights & Learnings
ðŸ”¹ Feature Engineering significantly impacts model performance
ðŸ”¹ Hyperparameter Tuning improved accuracy from 57.1% â†’ 64.3%
ðŸ”¹ Deep Learning overfitting â€“ model achieved 100% accuracy in training but performed poorly in validation
ðŸ”¹ Using Adam Optimizer instead of SGD could improve deep learning training efficiency


